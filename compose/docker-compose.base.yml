x-env: &env
  env_file: [ ./.env.common ]

networks:
  appnet:
    driver: bridge
    name: hudius-solar-stack_appnet

volumes:
  pg-data: {}
  mysql-data: {}
  minio-data: {}
  grafana-data: {}
  superset-data: {}
  ninjadb: {}
  loki-data: {}
  kafka-data: {}
  connect-data: {}
  redis-data: {}
  certbot-webroot: {}     # ACME HTTP-01 webroot
  letsencrypt: {}         # /etc/letsencrypt (인증서 저장소)
  prometheus-data: {}

services:
  # =========================
  # (임시) 인증 전용 Nginx
  # =========================
  acme-nginx:
    image: nginx:1.27-alpine
    profiles: ["acme"]
    restart: unless-stopped
    ports:
      - "59.0.196.26:80:80"   # 인증 단계에서 .26:80만 열어둠
    volumes:
      - certbot-webroot:/usr/share/nginx/html:ro
    networks: [ appnet ]

  # 최초 발급(1회 실행용)
  certbot-init:
    image: certbot/certbot:latest
    profiles: ["acme"]
    networks: [ appnet ]
    volumes:
      - certbot-webroot:/var/www/certbot
      - letsencrypt:/etc/letsencrypt
    # 셸 쓰지 말고, certbot 기본 entrypoint + JSON 배열 command 로!
    command: [
      "certonly","--webroot",
      "-w","/var/www/certbot",
      "-d","www.greenfesco.com",
      "-d","greenfesco.com",
      "--agree-tos","-m","hudius@hudius.com",
      "--non-interactive","--rsa-key-size","4096"
    ]

  # (운영) 자동 갱신
  certbot-renew:
    image: certbot/certbot:latest
    profiles: ["prod"]
    restart: unless-stopped
    networks: [ appnet ]
    volumes:
      - certbot-webroot:/var/www/certbot
      - letsencrypt:/etc/letsencrypt
    entrypoint: ["/bin/sh","-lc"]
    command: |
      while :; do
        certbot renew --webroot -w /var/www/certbot --quiet || true
        sleep 12h
      done
  # =========================
  # (운영) 리버스 프록시 Nginx
  # =========================
  nginx:
    image: nginx:1.27-alpine
    profiles: ["prod"]
    restart: unless-stopped
    depends_on:
      portal:                { condition: service_started }
      oauth2-proxy-mlflow:   { condition: service_started }
      minio:                 { condition: service_started }
      grafana:               { condition: service_started }
      superset:              { condition: service_started }
      keycloak:              { condition: service_started }
      vllm:                  { condition: service_started }
      prometheus:            { condition: service_started }
      loki:                  { condition: service_started }
    ports:
      - "59.0.196.26:80:80"
      - "59.0.196.26:443:443"
    volumes:
      - ../git/nginx/conf.d:/etc/nginx/conf.d:ro
      - ../git/portal/app/templates:/usr/share/nginx/html:ro
      - certbot-webroot:/var/www/certbot
      - letsencrypt:/etc/letsencrypt:ro
    networks: [ appnet ]
    dns: [ 127.0.0.11, 8.8.8.8 ]
    dns_search: .

  # =========================
  # 내부 서비스들 (운영 프로필)
  # =========================
  postgres:
    image: timescale/timescaledb:2.14.2-pg16
    <<: *env
    profiles: ["prod"]
    environment:
      POSTGRES_DB: solar_db
      POSTGRES_USER: hudius
      POSTGRES_PASSWORD: hudius2020!
      POSTGRES_ROOT_PASSWORD: hudius2020!
    volumes: [ pg-data:/var/lib/postgresql/data ]
    ports: [ "5433:5432" ]
    networks: [ appnet ]
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U hudius -d solar_db || exit 1"]
      interval: 10s
      timeout: 3s
      retries: 10

  mysql:
    image: mysql:8.0
    profiles: ["prod"]
    environment:
      MYSQL_ROOT_PASSWORD: hudius2020!
      MYSQL_DATABASE: invoiceninja
      MYSQL_USER: hudius
      MYSQL_PASSWORD: hudius2020!
    volumes: [ mysql-data:/var/lib/mysql ]
    ports: [ "3307:3306" ]
    networks: [ appnet ]
    healthcheck:
      test: ["CMD-SHELL", "mysqladmin ping -h localhost -u root -phudius2020! || exit 1"]
      interval: 10s
      timeout: 3s
      retries: 10

  redis:
    image: redis:8.2.0
    profiles: ["prod"]
    volumes: [ redis-data:/data ]
    ports: [ "6380:6379" ]
    networks: [ appnet ]
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 10

  kafka:
    image: bitnami/kafka:3.7
    <<: *env
    env_file: [ ./.env.kafka ]
    profiles: ["prod"]
    ports: [ "29092:9092" ]
    environment:
      - KAFKA_ENABLE_KRAFT=yes
      - KAFKA_CFG_PROCESS_ROLES=broker,controller
      - KAFKA_CFG_NODE_ID=1
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@kafka:9093
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
    volumes: [ kafka-data:/bitnami/kafka ]
    expose: [ "9092" ]
    networks: [ appnet ]
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics.sh --bootstrap-server localhost:9092 --list"]
      interval: 10s
      timeout: 3s
      retries: 20
      start_period: 30s
    restart: unless-stopped

  kafka-connect:
    image: quay.io/debezium/connect:2.7
    profiles: ["prod"]
    environment:
      - BOOTSTRAP_SERVERS=kafka:9092
      - GROUP_ID=connect-cluster
      - CONFIG_STORAGE_TOPIC=_connect_configs
      - OFFSET_STORAGE_TOPIC=_connect_offsets
      - STATUS_STORAGE_TOPIC=_connect_status
    volumes:
      - connect-data:/kafka/connect-data
      - ../git/kafka/connect:/kafka/connect:ro
    expose: [ "8083" ]
    depends_on:
      kafka:
        condition: service_healthy
    networks: [ appnet ]

  airflow-init:
    image: apache/airflow:3.0.3
    <<: *env
    env_file: [ ./.env.airflow ]
    profiles: ["prod"]
    entrypoint: /bin/bash
    command: -lc "airflow db migrate && /opt/airflow/init/init-users.sh"
    volumes:
      - ../git/airflow/dags:/opt/airflow/dags
      - ../git/airflow/plugins:/opt/airflow/plugins
      - ../git/airflow/requirements.txt:/opt/airflow/requirements.txt
      - ../git/airflow/init:/opt/airflow/init
    depends_on:
      postgres: { condition: service_healthy }
      redis:    { condition: service_healthy }
    networks: [ appnet ]
    restart: "no"

  airflow-api-server:
    image: apache/airflow:3.0.3
    <<: *env
    env_file: [ ./.env.airflow ]
    profiles: ["prod"]
    command: bash -lc "pip install -r /opt/airflow/requirements.txt || true && exec airflow api-server --proxy-headers"
    volumes:
      - ../git/airflow/dags:/opt/airflow/dags
      - ../git/airflow/plugins:/opt/airflow/plugins
      - ../git/airflow/requirements.txt:/opt/airflow/requirements.txt
    depends_on:
      airflow-init: { condition: service_completed_successfully }
      postgres:     { condition: service_healthy }
      redis:        { condition: service_healthy }
    ports: [ "58080:8080" ]
    networks: [ appnet ]
    environment: [ "FORWARDED_ALLOW_IPS=*" ]
    healthcheck:
      test: ["CMD","bash","-lc","curl -fsS http://localhost:8080/api/v2/version || curl -fsS http://localhost:8080/api/v1/version"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  airflow-dag-processor:
    image: apache/airflow:3.0.3
    <<: *env
    env_file: [ ./.env.airflow ]
    profiles: ["prod"]
    command: dag-processor
    volumes:
      - ../git/airflow/dags:/opt/airflow/dags
      - ../git/airflow/plugins:/opt/airflow/plugins
      - ../git/airflow/requirements.txt:/opt/airflow/requirements.txt
    depends_on:
      airflow-init: { condition: service_completed_successfully }
      postgres:     { condition: service_healthy }
      redis:        { condition: service_healthy }
    networks: [ appnet ]

  airflow-triggerer:
    image: apache/airflow:3.0.3
    <<: *env
    env_file: [ ./.env.airflow ]
    profiles: ["prod"]
    command: triggerer
    volumes:
      - ../git/airflow/dags:/opt/airflow/dags
      - ../git/airflow/plugins:/opt/airflow/plugins
      - ../git/airflow/requirements.txt:/opt/airflow/requirements.txt
    depends_on:
      airflow-init: { condition: service_completed_successfully }
      postgres:     { condition: service_healthy }
      redis:        { condition: service_healthy }
    networks: [ appnet ]

  portal:
    image: python:3.11-slim
    <<: *env
    env_file: [ ./.env.app ]
    profiles: ["prod"]
    working_dir: /srv/app
    command: >
      bash -lc "pip install --no-cache-dir -r requirements.txt &&
                 exec gunicorn solar.wsgi:application -c gunicorn.conf.py"
    volumes:
      - ../git/portal/app:/srv/app
      - ../git/portal/requirements.txt:/srv/app/requirements.txt
      - ../git/portal/gunicorn.conf.py:/srv/app/gunicorn.conf.py
      - ../git/feast:/srv/feast
    depends_on:
      postgres: { condition: service_healthy }
    expose: [ "8001" ]
    networks: [ appnet ]
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:8001/ || exit 1"]
      interval: 10s
      timeout: 3s
      retries: 30
      start_period: 120s

  mlflow:
    image: ghcr.io/mlflow/mlflow:v3.2.0
    <<: *env
    profiles: ["prod"]
    environment:
      - MLFLOW_BACKEND_URI=postgresql://hudius:hudius2020!@postgres:5432/solar_db
      - MLFLOW_ARTIFACTS_URI=s3://mlflow-artifacts
    command: >
      bash -lc "pip install --no-cache-dir psycopg2-binary &&
                 exec mlflow server
                   --backend-store-uri postgresql://hudius:hudius2020!@postgres:5432/solar_db
                   --artifacts-destination s3://mlflow-artifacts
                   --host 0.0.0.0
                   --port 5001"
    expose: [ "5001" ]
    networks: [ appnet ]
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:5001/ || exit 1"]
      interval: 10s
      timeout: 3s
      retries: 20
      start_period: 20s

  oauth2-proxy-mlflow:
    image: quay.io/oauth2-proxy/oauth2-proxy:v7.6.0
    profiles: ["prod"]
    command: >
      - oauth2-proxy
      - --http-address=0.0.0.0:4180
      - --provider=keycloak
      - --client-id=mlflow-client
      - --client-secret=your-client-secret
      - --cookie-secret=l9Niq/dpfsabn4/hSiKFsQ==
      - --keycloak-group=mlflow-users
      - --email-domain=*
      - --upstream=http://mlflow:5001
      - --cookie-secure=false
      - --cookie-httponly=true
      - --cookie-refresh=1h
      - --cookie-expire=4h
      - --http-address=0.0.0.0:4180
      - --reverse-proxy=true
      - --ssl-insecure-skip-verify=true
    depends_on:
      mlflow:   { condition: service_started }
      keycloak: { condition: service_started }
    expose: [ "4180" ]
    networks: [ appnet ]
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:4180/oauth2/start"]
      interval: 10s
      timeout: 3s
      retries: 20
      start_period: 10s

  minio:
    image: minio/minio:latest
    <<: *env
    env_file: [ ./.env.minio ]
    profiles: ["prod"]
    command: server /data --console-address ':9001'
    volumes: [ minio-data:/data ]
    expose: [ "9000", "9001" ]
    networks: [ appnet ]
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:9001/minio/health/ready || exit 1"]
      interval: 10s
      timeout: 3s
      retries: 20
      start_period: 10s

  grafana:
    image: grafana/grafana:11.4.0
    <<: *env
    env_file: [ ./.env.grafana ]
    profiles: ["prod"]
    volumes:
      - ../git/grafana/provisioning:/etc/grafana/provisioning
      - ../git/grafana/dashboards:/var/lib/grafana/dashboards
      - grafana-data:/var/lib/grafana
    expose: [ "3000" ]
    networks: [ appnet ]
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:3000/api/health || exit 1"]
      interval: 10s
      timeout: 3s
      retries: 20
      start_period: 15s

  superset:
    image: apache/superset:4.1.1
    profiles: ["prod"]
    command: >
      bash -lc "
        pip install --no-cache-dir psycopg2-binary &&
        pip install --no-cache-dir 'authlib>=1.3.0' &&
        /usr/bin/run-server.sh
      "
    volumes:
      - superset-data:/app/superset_home
      - ../git/superset/superset_config.py:/app/pythonpath/superset_config.py
      - ../git/superset/init:/docker-entrypoint-initdb.d
    depends_on:
      postgres: { condition: service_healthy }
    expose: [ "8088" ]
    networks: [ appnet ]
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8088/health || curl -sf http://localhost:8088/login/ || exit 1"]
      interval: 10s
      timeout: 3s
      retries: 30
      start_period: 20s

  keycloak:
    image: quay.io/keycloak/keycloak:26.3.2
    profiles: ["prod"]
    command: ["start-dev", "--import-realm", "--import-realm-strategy=OVERWRITE_EXISTING"]
    environment:
      KEYCLOAK_ADMIN: hudius
      KEYCLOAK_ADMIN_PASSWORD: hudius2020!
      KC_DB: "postgres"
      KC_DB_URL: "jdbc:postgresql://postgres:5432/solar_db"
      KC_DB_USERNAME: "hudius"
      KC_DB_PASSWORD: "hudius2020!"
      KC_HEALTH_ENABLED: "true"
      KC_METRICS_ENABLED: "true"
      KC_RUN_IN_CONTAINER: "true"
      KC_HTTP_ENABLED: "true"
      KC_HTTPS_ENABLED: "false"
      KC_HOSTNAME: "www.greenfesco.com"
    volumes:
      - ../git/keycloak/themes/hudius:/opt/keycloak/themes/hudius
      - ../git/keycloak/realm-export:/opt/keycloak/bin/../data/import:ro
    depends_on:
      postgres: { condition: service_healthy }
    ports: [ "8081:8080" ]
    networks: [ appnet ]
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:8080/realms/solar-stack || exit 1"]
      interval: 10s
      timeout: 3s
      retries: 30
      start_period: 20s
    restart: unless-stopped

  openmeter:
    image: ghcr.io/openmeterio/openmeter:latest
    <<: *env
    env_file: [ ./.env.openmeter ]
    profiles: ["prod"]
    environment:
      - OPENMETER_KAFKA_BROKERS=kafka:9092
      - OPENMETER_KAFKA_SECURITY_PROTOCOL=PLAINTEXT
      - KAFKA_BROKERS=kafka:9092
    volumes:
      - ../git/openmeter/config.yaml:/etc/openmeter/config.yaml
    expose: [ "8888" ]
    depends_on:
      kafka: { condition: service_healthy }
    networks: [ appnet ]
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8888/health || exit 1"]
      interval: 10s
      timeout: 3s
      retries: 20
      start_period: 10s

  invoiceninja:
    image: invoiceninja/invoiceninja:5
    <<: *env
    env_file: [ ./.env.invoiceninja ]
    profiles: ["prod"]
    volumes:
      - ninjadb:/var/www/app/storage
      - ../git/invoiceninja/env-overrides.env:/overrides.env
      - ../git/invoiceninja/config/.env:/var/www/app/.env
    depends_on:
      mysql: { condition: service_healthy }
    expose: [ "9000" ]
    networks: [ appnet ]
    healthcheck:
      test: ["CMD-SHELL", "bash -lc 'echo > /dev/tcp/localhost/9000'"]
      interval: 10s
      timeout: 3s
      retries: 20
      start_period: 10s

  vllm:
    image: vllm/vllm-openai:latest
    <<: *env
    env_file: [ ./.env.llm ]
    profiles: ["prod"]
    command: >
      --model ${LLM_MODEL_PATH_OR_ID}
      --dtype auto
      --port 8000
      --max-model-len 8192
    ports: [ "8000:8000" ]
    volumes:
      - /models:/models
      - ../git/llm/serving:/srv/llm
    expose: [ "8000" ]
    networks: [ appnet ]
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8000/v1/models || exit 1"]
      interval: 10s
      timeout: 3s
      retries: 30
      start_period: 20s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped

  dcgm-exporter:
    image: nvcr.io/nvidia/k8s/dcgm-exporter:3.3.6-3.4.2-ubuntu22.04
    profiles: ["prod"]
    restart: unless-stopped
    ports: [ "9400:9400" ]
    environment:
      NVIDIA_VISIBLE_DEVICES: "all"
      NVIDIA_DRIVER_CAPABILITIES: "utility,compute"
      DCGM_EXPORTER_LISTEN: ":9400"
    networks: [ appnet ]
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9400/metrics"]
      interval: 30s
      timeout: 5s
      retries: 10
      start_period: 30s
    runtime: nvidia
    privileged: true
    security_opt: [ "apparmor=unconfined", "seccomp=unconfined" ]
    devices:
      - /dev/nvidia0:/dev/nvidia0
      - /dev/nvidiactl:/dev/nvidiactl
      - /dev/nvidia-uvm:/dev/nvidia-uvm
      - /dev/nvidia-uvm-tools:/dev/nvidia-uvm-tools
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /sys/kernel/debug:/sys/kernel/debug:ro
      - /etc/nvidia:/etc/nvidia:ro

  promtail:
    image: grafana/promtail:2.9.8
    profiles: ["prod"]
    command: -config.file=/etc/promtail/config.yml
    volumes:
      - ../git/loki/promtail-config.yml:/etc/promtail/config.yml:ro
      - /var/log:/var/log:ro
    expose: [ "9080" ]
    networks: [ appnet ]
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:9080/ready || exit 1"]
      interval: 10s
      timeout: 3s
      retries: 20
      start_period: 10s

  loki:
    image: grafana/loki:2.9.8
    profiles: ["prod"]
    volumes: [ loki-data:/loki ]
    expose: [ "3100" ]
    networks: [ appnet ]
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:3100/ready || exit 1"]
      interval: 10s
      timeout: 3s
      retries: 20
      start_period: 10s

  prometheus:
    image: prom/prometheus:v3.5.0
    <<: *env
    env_file: [ ./.env.prometheus ]
    profiles: ["prod"]
    volumes:
      - ../git/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    expose: [ "9090" ]
    networks: [ appnet ]
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:9090/-/ready || exit 1"]
      interval: 10s
      timeout: 3s
      retries: 20
      start_period: 10s
